{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur and updated by Ross Beveridge.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/fall23/notebooks/module05_02_cross_validation.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating classifiers:  cross validation \n",
    "\n",
    "### Is more data better?\n",
    "\n",
    "\n",
    "Perhaps our primary goal as machine learning practictioners is generalization. We will find that generalization comes with a great deal of nuance - but let us start with some basics hinging upon the following question:\n",
    "\n",
    "> Does having more training samples improve performance on unseen (test) data? \n",
    "\n",
    "Intuitively, that answer would appear to be an obvious 'yes'; the more data we have available, the more accurate our classifiers should become.  To demonstrate this, let's evaluate a k-nearest neighbor classifier on a fixed test set with increasing number of training examples.  The resulting curve of accuracy as a function of number of examples is called a **learning curve**.\n",
    "\n",
    "Our first step is to create a fixed test set to evaluate the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X, y = load_digits(return_X_y=True)\n",
    "\n",
    "training_sizes = [20, 40, 100, 200, 400, 600, 800, 1000]\n",
    "\n",
    "# note the use of the stratify keyword: it makes it so that each \n",
    "# class is equally represented in both train and test set\n",
    "X_full_train, X_test, y_full_train, y_test = train_test_split(\n",
    "    X, y, test_size = len(y)-max(training_sizes), \n",
    "    stratify=y, random_state=1)\n",
    "\n",
    "print(f'We have selected {X_test.shape[0]} test images from {X.shape[0]} images total.')\n",
    "print(f'The remaining {X_full_train.shape[0]} images are available for training.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the next part we will generate different sized training sets from our available training data and evaluate the classifier trained on each training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "for training_size in training_sizes :\n",
    "    X_train,_ , y_train,_ = train_test_split(\n",
    "        X_full_train, y_full_train, test_size = \n",
    "        len(y_full_train)-training_size+10, stratify=y_full_train)\n",
    "    knn = KNeighborsClassifier(n_neighbors=1)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy.append(np.sum((y_pred==y_test))/len(y_test))\n",
    "    print(f'training size: {X_train.shape[0]} \\t accuracy: {accuracy[-1]:5.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(training_sizes, accuracy, 'ob')\n",
    "plt.xlabel('training set size')\n",
    "plt.ylabel('accuracy')\n",
    "plt.ylim((0.5,1));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection\n",
    "\n",
    "* What can you conclude from this plot?  \n",
    "* Why would you want to compute a learning curve on your data?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making better use of our data with cross validation\n",
    "\n",
    "The discussion above demonstrates that it is best to have as large of a training set as possible.  We also need to have a large enough test set, so that the accuracy estimates are accurate.  How do we balance these two contradictory requirements?  Cross-validation provides us a more effective way of using our data.  Here is the pseudo-code:\n",
    "\n",
    "**$k$-fold cross validation**\n",
    "\n",
    "* Randomly partition the data into $k$ subsets (\"folds\").\n",
    "* Set one fold aside for evaluation and train a model on the remaining $k$ folds and evaluate it on the held-out fold.\n",
    "* Repeat until each fold has been used for evaluation\n",
    "* Compute accuracy by averaging over the accuracy estimates generated for each fold.\n",
    "\n",
    "Here is an illustration of 8-fold cross validation:\n",
    "\n",
    "\n",
    "<img style=\"padding: 10px; float:left;\" alt=\"cross-validation by MBanuelos22 CC BY-SA 4.0\" src=\"https://upload.wikimedia.org/wikipedia/commons/c/c7/LOOCV.gif\" width=\"600\">\n",
    "\n",
    "As you can see, this procedure is more expensive than dividing your data into train and test set.  When dealing with relatively small datasets, which is when you want to use this procedure, this won't be an issue.\n",
    "\n",
    "Typically cross-validation is used with the number of folds being in the range of 5-10.  An extreme case is when the number of folds equals the number of training examples.  This special case is called *leave-one-out cross-validation*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the scikit-learn breast cancer dataset to demonstrate the use of cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "print('number of examples ', len(y))\n",
    "print('number of features ', len(X[0]))\n",
    "print('labels: ', data.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4, \n",
    "                                                    random_state=0)\n",
    "classifier = KNeighborsClassifier(n_neighbors=3)\n",
    "\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compute the accuracy of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_pred==y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compute accuracy using [cross_val_score](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3)\n",
    "accuracy = cross_val_score(classifier, X, y, cv=5, scoring='accuracy')\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This yields an array containing the accuracy values for each fold.\n",
    "When reporting your results, you will typically show the mean:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The arguments of `cross_val_score`:\n",
    "* A classifier (anything that satisfies the scikit-learn classifier API)\n",
    "* data (features/labels)\n",
    "* `cv` : an integer that specifies the number of folds (can be used in more sophisticated ways as we will see below).\n",
    "* `scoring`: this determines which accuracy measure is evaluated for each fold.  Here's a link to the  [list of available measures](https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter) in scikit-learn.\n",
    "\n",
    "You can obtain accuracy for other metrics.  *Balanced accuracy* for example, is appropriate when the data is unbalanced (e.g. when one class contains a much larger number of examples than other classes in the data)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = cross_val_score(classifier, X, y, cv=5, \n",
    "                           scoring='balanced_accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`cross_val_score` is somewhat limited, in that it simply returns a list of accuracy scores.  In practice, we often want to have more information about what happened during training, and also to compute multiple accuracy measures.\n",
    "[cross_validate](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_validate.html) will provide you with that information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = cross_validate(classifier, X, y, cv=5, \n",
    "                         scoring='accuracy', return_estimator=True)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The object returned by `cross_validate` is a Python dictionary as the output suggests.  To extract a specific piece of data from this object, simply access the dictionary with the appropriate key:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results['test_score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you would like to know the predictions made for each training example during cross-validation use [cross_val_predict](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_predict.html) instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "y_pred = cross_val_predict(classifier, X, y, cv=5)\n",
    "\n",
    "np.mean(y==y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Want more control?\n",
    "\n",
    "The above way of performing cross-validation doesn't always give us enough control over the process:  we usually want our machine learning experiments be reproducible, and to be able to use the same cross-validation splits with multiple algorithms.  The scikit-learn `KFold` and `StratifiedKFold` cross-validation generators are the way to achieve that.  \n",
    "\n",
    "`KFold` simply chooses a random subset of examples for each fold.  This strategy can lead to cross-validation folds in which the classes are not well-represented as the following toy example demonstrates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold, KFold\n",
    "\n",
    "X_toy = np.array([[1, 2], [3, 4], [5, 6], [7, 8], [9,10], [11, 12]])\n",
    "y_toy = np.array([0, 0, 1, 1, 1, 1])\n",
    "cv = KFold(n_splits=2, random_state=42, shuffle=True)\n",
    "# cv = StratifiedKFold(n_splits=2, random_state=42, shuffle=True)\n",
    "i  = 1\n",
    "for train_idx, test_idx in cv.split(X_toy, y_toy):\n",
    "    print(f'Split {i}')\n",
    "    print(f'\\t Test: {test_idx} Train: {train_idx}')\n",
    "    X_train, X_test = X_toy[train_idx], X_toy[test_idx]\n",
    "    y_train, y_test = y_toy[train_idx], y_toy[test_idx]\n",
    "    print(f'\\t Training labels are: {y_train}')\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stratified sampling using `StratifiedKFold`\n",
    "\n",
    "`StratifiedKFold` addresses this issue by making sure that each class is represented in each fold in proportion to its overall fraction in the data.  This is particularly important when one or more of the classes have few examples.\n",
    "\n",
    "`StratifiedKFold` and `KFold` generate folds that can be used in conjunction with the cross-validation methods we saw above.\n",
    "As an example, we will demonstrate the use of `StratifiedKFold` with `cross_val_score` on the breast cancer datast:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedKFold(n_splits=5, random_state=1, shuffle=True)\n",
    "accuracy = cross_val_score(classifier, X, y, cv=cv, \n",
    "                           scoring='accuracy')\n",
    "np.mean(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems, `StratifiedKFold` is the preferred strategy.  However, for regression problems `KFold` is the way to go.\n",
    "\n",
    "#### Question\n",
    "\n",
    "* Why is `KFold` used in regression probelms rather than `StratifiedKFold`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing cross-validation\n",
    "\n",
    "To clarify the distinction between the different methods of generating cross-validation folds and their different parameters let's look at the following figures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the code originated with the example in ...\n",
    "# https://scikit-learn.org/stable/auto_examples/model_selection/plot_cv_indices.html\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "np.random.seed(42)\n",
    "n_folds = 4\n",
    "\n",
    "# Generate the data\n",
    "X = np.random.randn(100, 10)\n",
    "\n",
    "# generate labels - classes 0,1,2 and 10,30,60 examples, respectively\n",
    "y = np.array([0] * 10 + [1] * 30 + [2] * 60)\n",
    "\n",
    "def plot_cv_indices(cv, X, y, ax, n_folds):\n",
    "    \"\"\"plot the indices of a cross-validation object.\"\"\"\n",
    "    line_width = 25\n",
    "    mark_size = 25\n",
    "    # Generate the training/testing visualizations for each CV split\n",
    "    for ii, (tr, tt) in enumerate(cv.split(X=X, y=y)):\n",
    "        # Fill in indices with the training/test groups\n",
    "        indices = np.zeros(len(X))\n",
    "        indices[tt] = 1\n",
    "        # Visualize the results\n",
    "        fold_cmap = ListedColormap(['darkgreen','orangered'])\n",
    "        ax.scatter(range(len(indices)), [ii] * len(indices),\n",
    "                   c=indices, marker=\"_\", lw=line_width, s=mark_size, \n",
    "                   cmap=fold_cmap)\n",
    "   \n",
    "    # Plot the data classes and groups at the end\n",
    "    labs_cmap = ListedColormap(['plum','purple','fuchsia'])\n",
    "    ax.scatter(range(len(X)), [ii+1] * len(X), c=y, marker='_', \n",
    "               s=mark_size, lw=line_width,\n",
    "               cmap=labs_cmap)\n",
    "\n",
    "   # Formatting\n",
    "    ax.set(ylim=[5,-1])\n",
    "    yticklabels = list(range(1,n_folds+1)) + ['class']\n",
    "    ax.set(yticks=np.arange(n_folds + 1), yticklabels=yticklabels)\n",
    "    ax.set(xlabel='index', ylabel=\"CV fold\")\n",
    "    ax.set_title('{}'.format(type(cv).__name__), fontsize=16)\n",
    "    return ax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the results of using `KFold` for fold generation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "cv = KFold(n_folds)\n",
    "plot_cv_indices(cv, X, y, ax, n_folds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this naive way of using `KFold` can lead to highly undesirable splits into cross-validation folds.\n",
    "Using `StratifiedKFold` addresses this to some extent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "cv = StratifiedKFold(n_folds)\n",
    "plot_cv_indices(cv, X, y, ax, n_folds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `StratifiedKFold` with shuffling of the examples is the preferred way of splitting the data into folds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(15,4))\n",
    "cv = StratifiedKFold(n_folds, shuffle=True)\n",
    "plot_cv_indices(cv, X, y, ax, n_folds);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question\n",
    "\n",
    "Consider the task of digitizing handwritten text (aka optical character recognition, or OCR).  For each letter in the alphabet you have multiple labeled examples generated by the same writer.  How would this setup affect the way you divide your examples into training and test sets, or when performing cross-validation?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leave One Out cross-validation\n",
    "\n",
    "A limiting case for k-fold cross validation is to have as many folds as there are are examples.  This results in what is called **leave-one-out cross-validation**. To be clear, how this works that given $N$  examples one builds $N$ models trained on a set from which **only one** example is removed from training and used for testing. \n",
    "\n",
    "The only situation you would use leave-one-out cross-validation is where you have an extremely small dataset.\n",
    "In some domains of interest, especially in medical data where it is often impossible to collect large number samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary and Discussion\n",
    "\n",
    "In this notebook we discussed cross-validation as a more effective way to make use of limited amounts of data compared to the strategy of splitting data into train and test sets.  For very large datasets where training is  time consuming you might still opt for evaluation on a single test set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This notebook is part of  course materials for CS 345: Machine Learning Foundations and Practice at Colorado State University.\n",
    "Original versions were created by Asa Ben-Hur.\n",
    "The content is availabe [on GitHub](https://github.com/asabenhur/CS345).*\n",
    "\n",
    "*The text is released under the [CC BY-SA license](https://creativecommons.org/licenses/by-sa/4.0/), and code is released under the [MIT license](https://opensource.org/licenses/MIT).*\n",
    "\n",
    "<img style=\"padding: 10px; float:right;\" alt=\"CC-BY-SA icon.svg in public domain\" src=\"https://upload.wikimedia.org/wikipedia/commons/d/d0/CC-BY-SA_icon.svg\" width=\"125\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github//asabenhur/CS345/blob/master/notebooks/exercise_notebooks/module04_ridge_regression_exercises.ipynb\">\n",
    "  <img align=\"left\" src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/>\n",
    "</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(0)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosave disabled\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "%matplotlib inline\n",
    "%autosave 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving ridge regression using gradient descent\n",
    "\n",
    "In this notebook you will implement ridge regression based on our implementation of linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The gradient for linear regression\n",
    "\n",
    "The mean-squared-error is our cost function for linear regression:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2 = \\frac{1}{N}\\sum_{i=1}^N(y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2 .\n",
    "$$\n",
    "\n",
    "For simplicity, we are ignoring the bias term $b$.  Before taking the gradient, we'll express it as:\n",
    "\n",
    "The gradient:\n",
    "\n",
    "\\begin{equation}\n",
    "\\begin{split}\n",
    "\\frac{\\partial J(\\mathbf{w})}{\\partial \\mathbf{w}} & = \n",
    "\\frac{1}{N}\\sum_{i=1}^N \\left( - 2y_i \\mathbf{x}_i + 2(\\mathbf{w}^\\top \\mathbf{x}_i)\\mathbf{x}_i \\right) \\\\\n",
    " & = -\\frac{1}{N}\\sum_{i=1}^N 2 \\left( y_i - \\mathbf{w}^\\top \\mathbf{x}_i\\right) \\mathbf{x}_i  .\n",
    "\\end{split}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "In the same way we were able to find the minimum of a univariate function using derivatives, we can find the minimum of a multi-dimensional surface by following the gradient.  This process is called **gradient descent**, and is described next.\n",
    "\n",
    "Given a function $J(\\mathbf{w})$, the gradient is the direction of steepest ascent.\n",
    "Therefore to minimize $J(\\mathbf{w})$, we iteratively take small steps in the direction of the negative of the gradient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient descent is an iterative algorithm as follows:\n",
    "\n",
    "* Initialize $\\mathbf{w}(0),\\eta$\n",
    "* for $t = 0,1,\\ldots$\n",
    "  * $\\mathbf{w}(t+1) = \\mathbf{w}(t) - \\eta \\frac{\\partial J}{\\partial \\mathbf{w}}$\n",
    "  * if converged, break\n",
    "  \n",
    "The halting condition for the algorithm can be specified in terms of the number of epochs like we did for the perceptron, or in terms of convergence of the loss function (the mean-squared-error).  In other words, if there is little change of the loss function across epochs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear regression using gradient descent\n",
    "\n",
    "We are now ready to implement linear regression using gradient descent, and apply it to the advertising data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((200, 2), (200,))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv('http://faculty.marshall.usc.edu/gareth-james/ISL/Advertising.csv', index_col=0)\n",
    "X = data[['TV', 'radio']].values\n",
    "y = data['sales'].values\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class linear_regression :\n",
    "    def __init__(self, learning_rate=0.00001, \n",
    "                 bias_learning_rate=0.01, epochs=1000) :\n",
    "        self.learning_rate = learning_rate\n",
    "        self.bias_learning_rate = bias_learning_rate\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def compute_loss(self, X, y):\n",
    "        squared_errors = np.power((self.predict(X) - y), 2)\n",
    "        return np.sum(squared_errors) / len(X)\n",
    "  \n",
    "    def predict(self, X) :\n",
    "        return X @ self.w + self.b\n",
    "                              \n",
    "    def fit(self, X, y):\n",
    "        self.w = np.zeros( (X.shape[1]), np.float)\n",
    "        self.b = 0.0\n",
    "        # self.loss stores a history of the loss function values\n",
    "        self.loss = []\n",
    "        for i in range(self.epochs):\n",
    "            y_pred = self.predict(X)\n",
    "            # compute individual gradients and then average them:\n",
    "            gradients_w = np.array([-2*(y[i]-y_pred[i])*X[i] \n",
    "                                    for i in range(len(X))])\n",
    "            gradient_w = np.mean(gradients_w, axis=0)\n",
    "            # updating w with the gradient\n",
    "            #print(gradient_w.shape)\n",
    "            self.w = self.w - self.learning_rate * gradient_w\n",
    "            gradient_b = - 2 * np.mean( (y - y_pred) )\n",
    "            #gradients_b = [-2*(y[i]-y_pred[i]) for i in range(len(X))]\n",
    "            #gradient_b = np.sum(gradients_b) / len(X)\n",
    "            # updating b with the gradient:\n",
    "            self.b = self.b - self.bias_learning_rate * gradient_b\n",
    "\n",
    "            self.loss.append(self.compute_loss(X, y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "linreg = linear_regression()\n",
    "linreg.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(5,3))\n",
    "plt.plot(linreg.loss[:50], 'b')\n",
    "plt.xlabel('epochs')\n",
    "plt.ylabel('loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's compare the learned parameters with the solution using the scikit-learn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.04575482 0.18799423] 2.9210999124051362\n",
      "[0.04552492 0.18527279] 3.030795322196597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# instantiate and fit\n",
    "linreg_sklearn = LinearRegression()\n",
    "linreg_sklearn.fit(X, y)\n",
    "\n",
    "# print the coefficients\n",
    "print (linreg_sklearn.coef_, linreg_sklearn.intercept_)\n",
    "print (linreg.w, linreg.b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ridge regression ($L_2$ regularization)\n",
    "\n",
    "Our orginal loss function\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2 = \\frac{1}{N}\\sum_{i=1}^N(y_i - \\mathbf{w}^\\top \\mathbf{x}_i)^2 \n",
    "$$\n",
    "\n",
    "is replaced by a regularized version that adds another term to the loss function:\n",
    "\n",
    "$$\n",
    "J(\\mathbf{w}) = \\frac{1}{N}\\sum_{i=1}^N(y_i-\\hat{y}_i)^2 + \\alpha \\,||\\mathbf{w}||^2.\n",
    "$$\n",
    "\n",
    "Here, $\\alpha>0$ is a parameter that controls the trade-off between fitting and regularization.\n",
    "The regularizer $\\mathbf{w}^\\top \\mathbf{w}$ is known as the $L_2$ regularizer.  Linear regression with the $L_2$ regularizer is known as *ridge regression*.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "* Derive the gradient for ridge regression.\n",
    "* Implement ridge regression by modifying the implementation of linear regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
